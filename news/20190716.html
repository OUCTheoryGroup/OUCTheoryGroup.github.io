<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh_cn" lang="zh_cn">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF8">
        <title>实验室新闻</title>
        <link rel="stylesheet" title="(NotSo) Simple" href="../files/notsosimple.css" type="text/css">


<body>

<!-- 顶部LOGO-->
<div id="header">
    <img src="../files/vis_lab_logo.png" width="900" alt="vision lab logo"><br><br>
</div>



<div id="main">
  <div class="content-mat">
    <div id="content">

      <!-- 侧栏部分  高峰 2018/07/28修改-->
      <div id="sidebar">      
      <br>
      <DL class="nav3-grid">
        <DT><h3><A  href="../index.html">       <BR>INTRODUCTION<BR> </A> </h3></DT>
        <DT><h3><A  href="../people.html">      <BR>MEMBERS     <BR> </A> </h3></DT>
        <DT><h3><A  href="../news.html">       <BR>NEWS     <BR> </A> </h3></DT>
        <DT><h3><A  href="../publication.html"> <BR>PUBLICATION <BR> </A> </h3></DT>
        <DT><h3><A  href="../link.html">        <BR>LINKS       <BR> </A> </h3></DT>
        <DT><h3><A  href="../album.html">       <BR>ALBUM       <BR> </A> </h3></DT>
      </DL>
      <br><br>
      </div>
      <!--  侧栏部分结束  -->


      <div id="wikitext">
      <h1>WELCOME to the VISION LAB @ Qingdao, OUC</h1>
      <hr>
      <p class="vspace"><br></p>

      <p align="center" style="line-height:2.0"><b>仲国强老师获BICS2019最佳会议论文</b></p>
      <p align="center"><small>更新时间: 20180201</small></p>
      
      <br>

      <p align="center"><img width=600 src=http://ww1.sinaimg.cn/large/6deb72a3ly1g5ft7h48cjj20qe0hjkjl.jpg></p>

      <p> 2019年7月13日至14日，第十届脑启发认知系统国际会议（The 10th International Conference on Brain-Inspired Cognitive Systems以下简称BICS 2019）在中国广州举行。会上，仲国强老师及其团队发表了论文“Long Short-Term Attention”（长短时注意模型），该论文被评为会议最佳论文。</p>

      <p>长短时记忆网络(LSTM) 是一种能够学习信息长期依赖关系的循环神经网络，它非常适用于处理序列数据相关的问题。传统的LSTM模型虽然能够记住序列信息，但不能特别注意到序列的重要部分。为了克服LSTM模型的不足并提高其性能，仲国强老师及其团队提出了一种新的序列学习模型--长短时注意(LSTA)模型。该模型将注意力机制嵌入到长短时记忆网络(LSTM)的记忆单元内部，它不仅可以处理长期和短期的依赖关系，还可以利用注意力机制关注到序列中的重要信息。论文中大量实验结果表明，在多个序列学习任务中LSTA模型的性能都优于LSTM和其它相关模型。基于该论文的科学价值和实验性能，最终该论文被评为最佳会议论文。</p>

      <p>BICS是脑启发的神经计算方面的重要国际会议，随着人工智能和类脑计算的快速发展，受到了全世界相关领域研究人员的广泛关注，今年共收到来自美国、英国、中国香港等国家和地区的投稿论文120余篇，100余人参会，经过两天的口头报告和墙报展示，大会评出最佳会议论文4篇和最佳墙报展示论文3篇。</p>


	  
      <br>
      <br>      


      <!--这里多加些断行防止页面过短-->
      <br><br><br><br>
      <br><br><br><br>
      <br><br><br><br>


    </div>
  </div>

  <div class="clearer"></div>
</div>

<div id="footer">
<p align="center"><small>联系VIS_LAB: (Email) feegao@aliyun.com (电话) +86-532-66781123.</small></p>
</div>

</body>
</html>